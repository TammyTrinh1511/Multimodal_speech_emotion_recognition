{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/dslab/code/tammy/speech_emotion/speech-emotion-recognition-speech-only.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdslab/home/dslab/code/tammy/speech_emotion/speech-emotion-recognition-speech-only.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdslab/home/dslab/code/tammy/speech_emotion/speech-emotion-recognition-speech-only.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdslab/home/dslab/code/tammy/speech_emotion/speech-emotion-recognition-speech-only.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# import matplotlib.pyplot as plt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdslab/home/dslab/code/tammy/speech_emotion/speech-emotion-recognition-speech-only.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import BertTokenizer,  BertForSequenceClassification, AdamW, BertConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from tensorboardX import SummaryWriter\n",
    "# from torchvggish import vggish, vggish_input\n",
    "import sys\n",
    "import random\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from utils import * \n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session1, #sum_file: 1820\n",
      "Session2, #sum_file: 3633\n",
      "Session3, #sum_file: 5769\n",
      "Session4, #sum_file: 7873\n",
      "Session5, #sum_file: 10043\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sessionID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/IEMOCAP_full_release/Session1/sentences/w...</td>\n",
       "      <td>Ses01F_impro01_F001</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/IEMOCAP_full_release/Session1/sentences/w...</td>\n",
       "      <td>Ses01F_impro01_F002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/IEMOCAP_full_release/Session1/sentences/w...</td>\n",
       "      <td>Ses01F_impro01_F003</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/IEMOCAP_full_release/Session1/sentences/w...</td>\n",
       "      <td>Ses01F_impro01_F004</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/IEMOCAP_full_release/Session1/sentences/w...</td>\n",
       "      <td>Ses01F_impro01_F005</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path            sessionID  \\\n",
       "0  data/IEMOCAP_full_release/Session1/sentences/w...  Ses01F_impro01_F001   \n",
       "1  data/IEMOCAP_full_release/Session1/sentences/w...  Ses01F_impro01_F002   \n",
       "2  data/IEMOCAP_full_release/Session1/sentences/w...  Ses01F_impro01_F003   \n",
       "3  data/IEMOCAP_full_release/Session1/sentences/w...  Ses01F_impro01_F004   \n",
       "4  data/IEMOCAP_full_release/Session1/sentences/w...  Ses01F_impro01_F005   \n",
       "\n",
       "   label  \n",
       "0      2  \n",
       "1      2  \n",
       "2     -1  \n",
       "3     -1  \n",
       "4      2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "list_files = []\n",
    "for x in range(5): \n",
    "    sess_name = \"Session\" + str(x+1)\n",
    "    path = 'data/IEMOCAP_full_release/' + sess_name + '/sentences/wav/'\n",
    "    file_search(path, list_files)\n",
    "    list_files = sorted(list_files)\n",
    "    print(sess_name + \", #sum_file: \" + str(len(list_files)))\n",
    "list_files_cut= [x.split('.')[-2].split('/')[-1] for x in list_files]\n",
    "df = pd.DataFrame({'path': list_files, 'sessionID': list_files_cut})\n",
    "df1 = pd.read_csv('speech_data/processed_digital_labels_head.csv')\n",
    "merged_df = pd.merge(df, df1, on='sessionID', how='inner')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio2spectrogram(filepath):\n",
    "    sample_rate, test_sound = wavfile.read(filepath, mmap=True)\n",
    "    print('sample rate', sample_rate)\n",
    "    _, spectrogram = log_specgram(test_sound, sample_rate)\n",
    "    print('spectrogram shape', spectrogram.shape)\n",
    "    print(type(spectrogram))\n",
    "    plt.imshow(spectrogram.T, aspect='auto', origin='lower')\n",
    "    return spectrogram\n",
    "\n",
    "def audio2wave(filepath):\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    sample_rate, test_sound = wavfile.read(filepath, mmap=True)\n",
    "    plt.plot(test_sound)\n",
    "\n",
    "def log_specgram(audio, sample_rate, window_size=40, step_size=20, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    print('nperseg', nperseg)\n",
    "    print('noverlap', noverlap)\n",
    "    freqs, _, spec = signal.spectrogram(audio, fs=sample_rate, window='hann', nperseg=nperseg, noverlap=noverlap, detrend=False)\n",
    "    return freqs, np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sr = librosa.load(list_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(data, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log mel spectrogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "spectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128, fmax=8000)\n",
    "log_spectrogram = librosa.power_to_db(spectrogram)\n",
    "librosa.display.specshow(log_spectrogram, y_axis='mel', sr=sr, x_axis='time')\n",
    "plt.title('Mel spectrogram')\n",
    "plt.colorbar(format='%+02.0f dB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=30)\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.subplot(3, 1, 1)\n",
    "librosa.display.specshow(mfcc, x_axis='time')\n",
    "plt.ylabel('MFCC')\n",
    "plt.colorbar()\n",
    "ipd.Audio(data, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate)\n",
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high=5)*1000)\n",
    "    return np.roll(data, shift_range)\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal audio\n",
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y=data, sr=sr)\n",
    "ipd.Audio(data, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio with noise\n",
    "x = noise(data)\n",
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stretched audio\n",
    "x = stretch(data)\n",
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifted audio\n",
    "x = shift(data)\n",
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitched audio \n",
    "x = pitch(data, sr)\n",
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y=x, sr=sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zcr(data, frame_length, hop_length):\n",
    "    zcr = librosa.feature.zero_crosssing_rate(data, frame_length=frame_length, hop_length=hop_length)\n",
    "    return np.squeeze(zcr)\n",
    "\n",
    "def rmse(data, frame_length, hop_length):\n",
    "    rmse = librosa.feature.rms(data, frame_length=frame_length, hop_length=hop_length)\n",
    "    return np.squeeze(rmse)\n",
    "\n",
    "def mfcc(data, sr, frame_length=2048, hop_length=512, flatten=True):\n",
    "    mfcc = librosa.feature.mfcc(data, sr)\n",
    "    return np.squeeze(mfcc.T) if not flatten else np.ravel(mfcc.T)\n",
    "\n",
    "def extract_features(data, sr=22050, frame_length=2048, hop_length=512):\n",
    "    result = np.array([])\n",
    "    result = np.hstack((result, rcr(data, frame_length, hop_length), \n",
    "                        rmse(data, frame_length, hop_length), \n",
    "                        mfcc(data, sr, frame_length, hop_length)))\n",
    "    return result\n",
    "\n",
    "def get_features(path, duration=2.5, offset=0.6):\n",
    "    data, sr = librosa.load(path, duration=duration, offset=offset)\n",
    "    aud = extract_features(data)\n",
    "    audio = np.array(aud)\n",
    "\n",
    "    noised_audio = noise(data)\n",
    "    aud2 = extract_features(noised_audio)\n",
    "    audio = np.vstack((audio, aud2))\n",
    "\n",
    "    pitched_audio = pitch(data, sr)\n",
    "    aud3 = extract_features(pitched_audio)\n",
    "    audio = np.vstack((audio, aud3))\n",
    "\n",
    "    pitched_audio1 = pitch(data, sr)\n",
    "    pitched_noised_audio = noise(pitched_audio1)\n",
    "    aud4 = extract_features(pitched_noised_audio)\n",
    "    audio = np.vstack((audio, aud4))\n",
    "    return audio\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  24\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp \n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features in parallel way \n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "def process_feature(path, emotion):\n",
    "    features = get_features(path)\n",
    "    X = []\n",
    "    Y = []\n",
    "    for ele in features: \n",
    "        X.append(ele)\n",
    "        Y.append(emotion)\n",
    "    return X, Y\n",
    "\n",
    "paths = merged_df['path']\n",
    "emotions = merged_df['label']\n",
    "results = Parallel(n_jobs=-1)(delayed(process_feature)(path, emotion) for path, emotion in zip(paths, emotions))\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for result in results: \n",
    "    X.extend(result[0])\n",
    "    Y.extend(result[1])\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_emo = pd.DataFrame(X)\n",
    "speech_emo['sessionID'] = merged_df['sessionID']\n",
    "speech_emo['label'] = Y\n",
    "speech_emo.to_csv('speech_data/speech_feature.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_emo = pd.read_csv('speech_data/speech_feature.csv')\n",
    "print(speech_emo.isna().any())\n",
    "speech_emo=speech_emo.fillna(0)\n",
    "print(speech_emo.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Emotions.iloc[: ,:-1].values\n",
    "Y = Emotions['Emotions'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.shape)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=42,test_size=0.2, shuffle=True)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train.reshape(x_train.shape[0] , x_train.shape[1] , 1)\n",
    "X_test = x_test.reshape(x_test.shape[0] , x_test.shape[1] , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelModel(nn.Module):\n",
    "    def __init__(self,num_emotions):\n",
    "        super().__init__()\n",
    "        # conv block\n",
    "        self.conv2Dblock = nn.Sequential(\n",
    "            # 1. conv block\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                       out_channels=16,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 2. conv block\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                       out_channels=32,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 3. conv block\n",
    "            nn.Conv2d(in_channels=32,\n",
    "                       out_channels=64,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 4. conv block\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                       out_channels=64,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3)\n",
    "        )\n",
    "        # Transformer block\n",
    "        self.transf_maxpool = nn.MaxPool2d(kernel_size=[2,4], stride=[2,4])\n",
    "        transf_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=512, dropout=0.4, activation='relu')\n",
    "        self.transf_encoder = nn.TransformerEncoder(transf_layer, num_layers=4)\n",
    "        # Linear softmax layer\n",
    "        self.out_linear = nn.Linear(320,num_emotions)\n",
    "        self.dropout_linear = nn.Dropout(p=0)\n",
    "        self.out_softmax = nn.Softmax(dim=1)\n",
    "    def forward(self,x):\n",
    "        # conv embedding\n",
    "        conv_embedding = self.conv2Dblock(x) #(b,channel,freq,time)\n",
    "        conv_embedding = torch.flatten(conv_embedding, start_dim=1) # do not flatten batch dimension\n",
    "        # transformer embedding\n",
    "        x_reduced = self.transf_maxpool(x)\n",
    "        x_reduced = torch.squeeze(x_reduced,1)\n",
    "        x_reduced = x_reduced.permute(2,0,1) # requires shape = (time,batch,embedding)\n",
    "        transf_out = self.transf_encoder(x_reduced)\n",
    "        transf_embedding = torch.mean(transf_out, dim=0)\n",
    "        # concatenate\n",
    "        complete_embedding = torch.cat([conv_embedding, transf_embedding], dim=1) \n",
    "        # final Linear\n",
    "        output_logits = self.out_linear(complete_embedding)\n",
    "        output_logits = self.dropout_linear(output_logits)\n",
    "        output_softmax = self.out_softmax(output_logits)\n",
    "        return output_logits, output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tammy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
