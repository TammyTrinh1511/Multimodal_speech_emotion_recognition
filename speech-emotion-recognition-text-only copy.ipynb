{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install transformers\n","# !pip install transformers[torch]\n","# !pip install tensorboardX\n","# !pip install pytorch_lightning"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pickle\n","import numpy as np\n","# import matplotlib.pyplot as plt\n","import pandas as pd\n","import os\n","from transformers import BertTokenizer,  BertForSequenceClassification, AdamW, BertConfig\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils import data\n","from tensorboardX import SummaryWriter\n","# from torchvggish import vggish, vggish_input\n","import sys\n","import random\n","import csv\n","from sklearn.metrics import confusion_matrix,classification_report\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","from datetime import datetime"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def file_search(dirname, ret, list_avoid_dir=[]): \n","    filenames = os.listdir(dirname)\n","    for filename in filenames: \n","        full_filename = os.path.join(dirname, filename)\n","        if os.path.isdir(full_filename):\n","            if full_filename.split(\"/\")[-1] in list_avoid_dir:\n","                continue\n","            else: \n","                file_search(full_filename, ret, list_avoid_dir)\n","        else: \n","            ret.append(full_filename)\n","            \n","def find_encoding(filename):\n","    rawdata = open(filename, 'rb').read()\n","    result = chardet.detect(rawdata)\n","    charenc = result['encoding']\n","    \n","def create_folder(dir_name): \n","    if not os.path.exists(dir_name): \n","        os.makedirs(dir_name)\n","        \n","def extract_trans(list_in_file, out_file):\n","    lines = []\n","    for in_file in list_in_file:\n","        cnt = 0\n","        encodings_to_try = ['latin-1', 'ISO-8859-1', 'utf-16']\n","        lines = None\n","        \n","        for encoding in encodings_to_try:\n","            try:\n","                with open(in_file, 'r', encoding=encoding) as f1:\n","                    lines = f1.readlines()\n","                break  # If successful, exit the loop\n","            except UnicodeDecodeError:\n","                print(f\"Failed to decode {in_file} with {encoding} encoding. Trying another encoding.\")\n","                continue\n","        \n","        with open(out_file, 'a', encoding='latin-1') as f2:  # Use 'a' for append mode\n","            csv_writer = csv.writer(f2)\n","            lines = sorted(lines)\n","            \n","            for line in lines:\n","                name = line.split(':')[0].split(' ')[0].strip()\n","                if name[:3] != 'Ses': \n","                    continue\n","                elif name[-3: -1] == 'XX': \n","                    continue\n","                trans = line.split(':')[1].strip()\n","                cnt += 1\n","                csv_writer.writerow([name, trans])\n","                    \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["list_files = []\n","for x in range(5):\n","    sess_name = 'Session' + str(x + 1)\n","    path = 'data/IEMOCAP_full_release'\n","    file_search(path, list_files)\n","    print(sess_name + ', #sum files: ' + str(len(list_files)))\n","    \n","extract_trans(list_files, 'processed_trans.csv')\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["file = pd.read_csv(\"processed_trans.csv\")\n","headerList = ['sessionID', 'text']\n","  \n","# converting data frame to csv\n","file.to_csv(\"processed_trans_head.csv\", header=headerList, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def find_category(lines):\n","    list_category = ['ang', 'hap', 'sad', 'neu', 'fru', 'exc', 'fea', 'sur', 'dis', 'oth', 'xxx']\n","    category = {}\n","    for cate in list_category: \n","        if category.__contains__(cate):\n","            pass\n","        else: \n","            category[cate] = len(category)\n","    is_target = True\n","    id = ''\n","    c_label = ''\n","    list_ret = []\n","    for line in lines: \n","        if is_target == True: \n","            try: \n","                id = line.split('\\t')[1].strip()\n","                label = line.split('\\t')[2].strip()\n","                if not category.__contains__(label):\n","                    print('ERROR: we can\\'t find ', label)\n","                    sys.exit()\n","                list_ret.append([id, label])\n","                is_target = False\n","            except: \n","                print('ERROR ', lone)\n","                sys.exit()\n","        else:\n","            if line == '\\n':\n","                is_target = True\n","    return list_ret\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def extract_labels(list_in_file, out_file) :\n","    id = ''\n","    lines = []\n","    list_ret = []\n","    \n","    for in_file in list_in_file:\n","        \n","        with open(in_file, 'r') as f1:\n","            lines = f1.readlines()\n","            lines = lines[2:]                           # remove head\n","            list_ret = find_category(lines)\n","            \n","        list_ret = sorted(list_ret)                   # sort based on first element\n","    \n","        with open(out_file, 'a') as f2:\n","            csv_writer = csv.writer(f2)\n","            csv_writer.writerows(list_ret)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["list_files = []\n","list_avoid_dir = ['Attribute', 'Categorical', 'Self-evaluation']\n","for x in range(5): \n","    sess_name = \"Session\" + str(x + 1)\n","    path = 'data/IEMOCAP_full_release/' + sess_name + '/dialog/EmoEvaluation/'\n","    file_search(path, list_files, list_avoid_dir)\n","    list_files = sorted(list_files)\n","    print(sess_name + \", #sum files: \" + str(len(list_files)))\n","extract_labels(list_files, \"processed_labels.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["file = pd.read_csv(\"processed_labels.csv\")\n","file.to_csv(\"processed_labels_head.csv\", header=['sessionID', 'label'], index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.title('Count of Emotions', size=16)\n","sns.countplot(file['label'])\n","plt.ylabel('Count', size=12)\n","plt.xlabel('Emotions', size=12)\n","sns.despine(top=True, right=True, left=False, bottom=False)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df1 = pd.read_csv('processed_labels_head.csv')\n","df1.loc[df1[\"label\"] == \"ang\", \"label\"] = 0\n","df1.loc[df1[\"label\"] == \"hap\", \"label\"] = 1\n","df1.loc[df1[\"label\"] == \"exc\", \"label\"] = 1\n","df1.loc[df1[\"label\"] == \"sad\", \"label\"] = 1\n","df1.loc[df1[\"label\"] == \"neu\", \"label\"] = 2\n","df1.loc[df1[\"label\"] == \"fru\", \"label\"] = 3\n","df1.loc[df1[\"label\"] == \"fea\", \"label\"] = -1\n","df1.loc[df1[\"label\"] == \"sur\", \"label\"] = -1\n","df1.loc[df1[\"label\"] == \"dis\", \"label\"] = -1\n","df1.loc[df1[\"label\"] == \"oth\", \"label\"] = -1\n","df1.loc[df1[\"label\"] == \"xxx\", \"label\"] = -1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df1.to_csv(\"processed_digital_labels_head.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data1 = pd.read_csv(\"processed_trans_head.csv\")\n","data2 = pd.read_csv(\"processed_digital_labels_head.csv\")\n","translabels = pd.merge(data1, data2, on = 'sessionID' , how = 'inner')\n","translabels.to_csv(\"processed_trans_labels_head.csv\")\n","# data1.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["list_files = []\n","for x in range(5): \n","    sess_name = \"Session\" + str(x+1)\n","    path = 'data/IEMOCAP_full_release/' + sess_name + '/sentences/wav/'\n","    file_search(path, list_files)\n","    list_files = sorted(list_files)\n","    print(sess_name + \", #sum_file: \" + str(len(list_files)))\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv('processed_trans_labels_head.csv')\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["docs = []\n","for text, label in zip(df['text'], df['label']): \n","    if label != -1: \n","        docs.append({'text': text, 'label': label})\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# no_rows = len(list_files)\n","# index = 0\n","# sprectrogram_shape = []\n","# docs = []\n","# bookmark = 0\n","# extraLabel = 0 \n","# for file in list_files: \n","#     if file.split('/')[-1].endswith('.wav'):\n","#         filename = file.split('/')[-1].strip('.wav')\n","#         label = df.loc[df['sessionID'] == filename]['label'].values[0]\n","#         text = df.loc[df['sessionID'] == filename]['text'].values[0]\n","#         if label != -1: \n","#             input_batch = "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["random.shuffle(docs)\n","random.shuffle(docs)\n","random.shuffle(docs)\n","total_len = len(docs)\n","train_len = int(0.8 * total_len)\n","train_list = docs[: train_len]\n","test_list = docs[train_len: ]\n","print('no of items for train', len(train_list))\n","print('no of items for test', len(test_list))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(len(train_list)):\n","    train_list[i]['label'] = int(train_list[i]['label'])\n","#     print(train_list[i]['label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir ./"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n"," \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", \n","    num_labels = 4,   \n","    output_attentions = False,\n","    output_hidden_states = False, \n",")\n","print(model)\n","params = list(model.named_parameters())\n","# optimizer = AdamW(model.parameters(),\n","#                   lr = 2e-5,\n","#                   eps = 1e-8 \n","#                 )\n","# from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","NUM_EPOCHS=4\n","\n","\n","writer = SummaryWriter(log_dir='results')\n","total_steps = len(train_list) * NUM_EPOCHS\n","\n","# Create the learning rate scheduler.\n","import torch.optim as optim\n","optimizer = optim.Adam(params=model.parameters(), lr=0.0001)\n","criterion = nn.CrossEntropyLoss()\n","lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","start_epoch = 0\n","total_steps = 1\n","NUM_EPOCHS= 4 \n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model.train()\n","model.to('cuda')\n","for epoch in tqdm(range(start_epoch, NUM_EPOCHS)):\n","    print(\"*\"*80)\n","    print(\"Epochs:\", epoch)\n","    print(\"*\"*80)\n","    lr_scheduler.step()\n","    random.shuffle(train_list)\n","    for every_trainlist in train_list:\n","        label1=every_trainlist['label']\n","        text=every_trainlist['text']\n","        label1=torch.tensor([label1])\n","        optimizer.zero_grad()\n","        input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0) \n","        label1=label1.to(device)\n","        input_ids=input_ids.to(device)\n","#         print(input_ids)\n","        output = model(input_ids, labels=label1)\n","        loss, logits = output[0], output[1]\n","#         loss = criterion(output, label1)\n","#         print('loss',loss.item())\n","        loss.backward()\n","        optimizer.step()\n","        _, preds = torch.max(logits, 1)\n","        accuracy = torch.sum(preds == label1)\n","#         print('accuracy.item()',accuracy.item())\n","        if total_steps % 10 == 0:\n","            with torch.no_grad():\n","                _, preds = torch.max(logits, 1)\n","                accuracy = torch.sum(preds == label1)\n","                writer.add_scalar('loss', loss.item(), total_steps)\n","                writer.add_scalar('accuracy', accuracy.item(), total_steps)                     \n","        total_steps+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y = []\n","y_pred = []\n","model.to('cpu')\n","model.eval()\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","for every_test_list in test_list: \n","    label1 = every_test_list['label']\n","    label1 = torch.tensor([label1])\n","    text = every_test_list['text']\n","    input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)\n","    with torch.no_grad():\n","        loss, output = model(input_ids, labels=label1)\n","        _, preds = torch.max(output, 1)\n","        y.append(label.numpy()[0])\n","        y_pred.append(preds.numpy()[0])\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_actu, y_pred)\n","print(cm)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cmn = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n","\n","ax = plt.subplots(figsize=(8, 5.5))[1]\n","sns.heatmap(cmn, cmap='flare', annot=True, square=True, linecolor='black', linewidths=0.75, ax = ax, fmt = '.2f', annot_kws={'size': 16})\n","ax.set_xlabel('Predicted', fontsize=18, fontweight='bold')\n","ax.xaxis.set_label_position('bottom')\n","ax.xaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n","ax.set_ylabel('Ground Truth', fontsize=18, fontweight='bold')\n","ax.yaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["classification_report(y, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# torch.save(model, '/kaggle/working/model_text.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":4}
